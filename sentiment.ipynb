{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ObjectMatrix/google-colab-notebook/blob/main/sentiment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "blGJpFg7a2Y7"
      },
      "outputs": [],
      "source": [
        "!pip install transformers\n",
        "!pip install pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D0iQoGG9vPTg"
      },
      "outputs": [],
      "source": [
        "!pip install numpy pandas tensorflow matplotlib seaborn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ebaBHkoewEPh"
      },
      "outputs": [],
      "source": [
        "!pip install emoji "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import pandas as pd\n",
        "# from transformers import pipeline\n",
        "\n",
        "# path = \"/content/drive/MyDrive/sentiment/sentiment.xlsx\"\n",
        "# df_sentiment = pd.read_excel(path)\n",
        "# comment_likes = df_sentiment[[\"likes\", \"comment\"]]\n",
        "\n",
        "# # Load the sentiment analysis model\n",
        "# sentiment_analysis = pipeline(\"sentiment-analysis\")\n",
        "\n",
        "# # Open a file to write the results\n",
        "# output_file = \"/content/drive/MyDrive/sentiment/sentiment_results.txt\"\n",
        "# with open(output_file, \"w\") as f:\n",
        "#     # Iterate over the rows of the df_sentiment dataframe\n",
        "#     for index, row in df_sentiment.iterrows():\n",
        "#         comment = row[\"comment\"]\n",
        "#         likes = row[\"likes\"]\n",
        "\n",
        "#         # Analyze sentiment of the comment\n",
        "#         result = sentiment_analysis(comment)\n",
        "\n",
        "#         # Get the sentiment prediction\n",
        "#         sentiment_label = result[0]['label']\n",
        "#         sentiment_score = result[0]['score']\n",
        "\n",
        "#         # Adjust sentiment score based on likes\n",
        "#         weighted_score = sentiment_score * (likes / 100)\n",
        "\n",
        "#         # Write the sentiment prediction with weighted score to the file\n",
        "#         f.write(f\"Comment: {comment}\\n\")\n",
        "#         f.write(f\"Sentiment: {sentiment_label}\\n\")\n",
        "#         f.write(f\"Original Score: {sentiment_score}\\n\")\n",
        "#         f.write(f\"Likes: {likes}\\n\")\n",
        "#         f.write(f\"Weighted Score: {weighted_score}\\n\")\n",
        "#         f.write(\"\\n\")\n"
      ],
      "metadata": {
        "id": "-mSakBIs5ZH6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Logistic **Regression**"
      ],
      "metadata": {
        "id": "oNOUQXEvMPDp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from transformers import pipeline, AutoTokenizer\n",
        "\n",
        "path = \"/content/drive/MyDrive/sentiment/sentiment.xlsx\"\n",
        "df_sentiment = pd.read_excel(path)\n",
        "comment_likes = df_sentiment[[\"likes\", \"comment\"]]\n",
        "\n",
        "# Load the sentiment analysis model\n",
        "sentiment_analysis = pipeline(\"sentiment-analysis\")\n",
        "\n",
        "# Load the tokenizer corresponding to the sentiment analysis model\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
        "\n",
        "# Open a file to write the results\n",
        "output_file = \"/content/drive/MyDrive/sentiment/sentiment_results.txt\"\n",
        "with open(output_file, \"w\") as f:\n",
        "    # Iterate over the rows of the df_sentiment dataframe\n",
        "    for index, row in df_sentiment.iterrows():\n",
        "        comment = row[\"comment\"]\n",
        "        likes = row[\"likes\"]\n",
        "\n",
        "        # Tokenize the comment, get input ids\n",
        "        input_ids = tokenizer.encode(comment, truncation=False)\n",
        "\n",
        "        # Split into chunks of max_length tokens\n",
        "        chunks = []\n",
        "        for i in range(0, len(input_ids), 512 - 2):  # we subtract 2 for [CLS] and [SEP]\n",
        "            chunks.append(input_ids[i:i + 512 - 2])\n",
        "\n",
        "        # Analyze sentiment of each chunk and average the scores\n",
        "        sentiment_scores = []\n",
        "        for chunk in chunks:\n",
        "            # Convert input ids back to text\n",
        "            chunk_text = tokenizer.decode(chunk)\n",
        "            result = sentiment_analysis(chunk_text)\n",
        "            sentiment_scores.append(result[0]['score'])\n",
        "        sentiment_score = sum(sentiment_scores) / len(sentiment_scores)\n",
        "        \n",
        "        # Get the sentiment prediction of the first chunk as the overall prediction\n",
        "        first_chunk_text = tokenizer.decode(chunks[0])\n",
        "        sentiment_label = sentiment_analysis(first_chunk_text)[0]['label']\n",
        "\n",
        "        # Adjust sentiment score based on likes\n",
        "        weighted_score = sentiment_score * (likes / 100)\n",
        "\n",
        "        # Write the sentiment prediction with weighted score to the file\n",
        "        f.write(f\"Comment: {comment}\\n\")\n",
        "        f.write(f\"Sentiment: {sentiment_label}\\n\")\n",
        "        f.write(f\"Original Score: {sentiment_score}\\n\")\n",
        "        f.write(f\"Likes: {likes}\\n\")\n",
        "        f.write(f\"Weighted Score: {weighted_score}\\n\")\n",
        "        f.write(\"\\n\")\n"
      ],
      "metadata": {
        "id": "Ji02cGm26TPO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**POSITIVE-NEGATIVE-NEUTRAL:** This Model does better"
      ],
      "metadata": {
        "id": "ru-BIxuO-HsU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from transformers import pipeline, AutoTokenizer\n",
        "\n",
        "path = \"/content/drive/MyDrive/sentiment/sentiment.xlsx\"\n",
        "df_sentiment = pd.read_excel(path)\n",
        "comment_likes = df_sentiment[[\"likes\", \"comment\"]]\n",
        "\n",
        "# Load the sentiment analysis model\n",
        "model_name = \"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
        "sentiment_analysis = pipeline(\"sentiment-analysis\", model=model_name)\n",
        "\n",
        "# Load the tokenizer corresponding to the sentiment analysis model\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Open a file to write the results\n",
        "output_file = \"/content/drive/MyDrive/sentiment/sentiment_results_neutral.txt\"\n",
        "with open(output_file, \"w\") as f:\n",
        "    # Iterate over the rows of the df_sentiment dataframe\n",
        "    for index, row in df_sentiment.iterrows():\n",
        "        comment = row[\"comment\"]\n",
        "        likes = row[\"likes\"]\n",
        "\n",
        "        # Tokenize the comment, get input ids\n",
        "        input_ids = tokenizer.encode(comment, truncation=False)\n",
        "\n",
        "        # Split into chunks of max_length tokens\n",
        "        chunks = []\n",
        "        for i in range(0, len(input_ids), 512 - 2):  # we subtract 2 for [CLS] and [SEP]\n",
        "            chunks.append(input_ids[i:i + 512 - 2])\n",
        "\n",
        "        # Analyze sentiment of each chunk and average the scores\n",
        "        sentiment_scores = []\n",
        "        sentiment_labels = []\n",
        "        for chunk in chunks:\n",
        "            # Convert input ids back to text\n",
        "            chunk_text = tokenizer.decode(chunk)\n",
        "            result = sentiment_analysis(chunk_text)\n",
        "            sentiment_scores.append(result[0]['score'])\n",
        "            sentiment_labels.append(result[0]['label'])\n",
        "        sentiment_score = sum(sentiment_scores) / len(sentiment_scores)\n",
        "        \n",
        "        # Get the sentiment prediction of the first chunk as the overall prediction\n",
        "        first_chunk_text = tokenizer.decode(chunks[0])\n",
        "        sentiment_label = sentiment_analysis(first_chunk_text)[0]['label']\n",
        "\n",
        "        # Adjust sentiment score based on likes\n",
        "        weighted_score = sentiment_score * (likes / 100)\n",
        "\n",
        "        # Write the sentiment prediction with weighted score to the file\n",
        "        f.write(f\"Comment: {comment}\\n\")\n",
        "        f.write(f\"Sentiment: {sentiment_label}\\n\")\n",
        "        f.write(f\"Original Score: {sentiment_score}\\n\")\n",
        "        f.write(f\"Likes: {likes}\\n\")\n",
        "        f.write(f\"Weighted Score: {weighted_score}\\n\")\n",
        "        f.write(\"\\n\")\n"
      ],
      "metadata": {
        "id": "kSZiM1g996Up"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**GRAPH**"
      ],
      "metadata": {
        "id": "VoZh6SX1COqZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from transformers import pipeline, AutoTokenizer\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "\n",
        "path = \"/content/drive/MyDrive/sentiment/sentiment.xlsx\"\n",
        "df_sentiment = pd.read_excel(path)\n",
        "comment_likes = df_sentiment[[\"likes\", \"comment\"]]\n",
        "\n",
        "# Load the sentiment analysis model\n",
        "model_name = \"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
        "sentiment_analysis = pipeline(\"sentiment-analysis\", model=model_name)\n",
        "\n",
        "# Load the tokenizer corresponding to the sentiment analysis model\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Open a file to write the results\n",
        "output_file = \"/content/drive/MyDrive/sentiment/sentiment_results_neutral.txt\"\n",
        "\n",
        "# A list to store sentiment labels for later use in plotting\n",
        "sentiment_labels = []\n",
        "\n",
        "with open(output_file, \"w\") as f:\n",
        "    # Iterate over the rows of the df_sentiment dataframe\n",
        "    for index, row in df_sentiment.iterrows():\n",
        "        comment = row[\"comment\"]\n",
        "        likes = row[\"likes\"]\n",
        "\n",
        "        # Tokenize the comment, get input ids\n",
        "        input_ids = tokenizer.encode(comment, truncation=False)\n",
        "\n",
        "        # Split into chunks of max_length tokens\n",
        "        chunks = []\n",
        "        for i in range(0, len(input_ids), 512 - 2):  # we subtract 2 for [CLS] and [SEP]\n",
        "            chunks.append(input_ids[i:i + 512 - 2])\n",
        "\n",
        "        # Analyze sentiment of each chunk and average the scores\n",
        "        sentiment_scores = []\n",
        "        for chunk in chunks:\n",
        "            # Convert input ids back to text\n",
        "            chunk_text = tokenizer.decode(chunk)\n",
        "            result = sentiment_analysis(chunk_text)\n",
        "            sentiment_scores.append(result[0]['score'])\n",
        "            sentiment_labels.append(result[0]['label'])\n",
        "        sentiment_score = sum(sentiment_scores) / len(sentiment_scores)\n",
        "        \n",
        "        # Get the sentiment prediction of the first chunk as the overall prediction\n",
        "        first_chunk_text = tokenizer.decode(chunks[0])\n",
        "        sentiment_label = sentiment_analysis(first_chunk_text)[0]['label']\n",
        "\n",
        "        # Adjust sentiment score based on likes\n",
        "        weighted_score = sentiment_score * (likes / 100)\n",
        "        sentiment_score_with_weight = sentiment_score + weighted_score\n",
        "        # Write the sentiment prediction with weighted score to the file\n",
        "        f.write(f\"Comment: {comment}\\n\")\n",
        "        f.write(f\"Sentiment: {sentiment_label}\\n\")\n",
        "        f.write(f\"sentimentPlusWeight: {sentiment_score_with_weight}\\n\")\n",
        "        f.write(f\"Original Score: {sentiment_score}\\n\")\n",
        "        f.write(f\"Likes: {likes}\\n\")\n",
        "        f.write(f\"Weighted Score: {weighted_score}\\n\")\n",
        "        f.write(\"\\n\")\n",
        "\n",
        "# Count the sentiment labels\n",
        "label_counts = Counter(sentiment_labels)\n",
        "\n",
        "# Plot a bar chart of the label counts\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(label_counts.keys(), label_counts.values())\n",
        "plt.title('Sentiment Analysis')\n",
        "plt.xlabel('Sentiment')\n",
        "plt.ylabel('Count')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "nXLFF4p-CRn9"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1VQ7LJYrt7qJiVeui_TVoKEY2tDH6b-Ag",
      "authorship_tag": "ABX9TyMMDcS/SO2axykmK08VLYro",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}