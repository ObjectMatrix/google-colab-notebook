{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "mount_file_id": "1KdDqtpC40LMZS-0GziPuyqr0jTjlr7RB",
      "authorship_tag": "ABX9TyOjGhHIpJhi3D1Z/QAIYK8Z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ObjectMatrix/google-colab-notebook/blob/main/embeddings_pdf_w_openai.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Keys\n",
        "import os\n",
        "import json\n",
        "\n",
        "drive_mount_path = '/content/drive'\n",
        "\n",
        "# Check if the drive is already mounted\n",
        "if not os.path.ismount(drive_mount_path):\n",
        "    from google.colab import drive\n",
        "    drive.mount(drive_mount_path)\n",
        "else:\n",
        "    print(\"Drive is already mounted.\")\n",
        "with open('/content/drive/MyDrive/secrets.json', 'r') as f:\n",
        "  secrets = json.load(f)\n",
        "KEY=secrets['SECRET_KEY']\n",
        "os.environ[\"OPENAI_API_KEY\"] = KEY"
      ],
      "metadata": {
        "id": "gWLd4BopZkbE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "outputId": "d550c614-6c09-40ad-c752-86ae1fca74cb"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive is already mounted.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Requirement\n",
        "!pip install openai -q\n",
        "!pip install langchain -q\n",
        "!pip install chromadb -q\n",
        "!pip install tiktoken -q\n",
        "!pip install pypdf -q\n",
        "!pip install unstructured[local-inference] -q\n",
        "!pip install gradio -q\n",
        "\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "llm = ChatOpenAI(temperature=0,model_name=\"gpt-4\")\n",
        "\n",
        "# Data Ingestion\n",
        "from langchain.document_loaders import DirectoryLoader\n",
        "pdf_loader = DirectoryLoader('/content/drive/MyDrive/pdfs/', glob=\"**/*.pdf\")\n",
        "excel_loader = DirectoryLoader('/content/drive/MyDrive/pdfs/', glob=\"**/*.txt\")\n",
        "word_loader = DirectoryLoader('/content/drive/MyDrive/pdfs/', glob=\"**/*.docx\")\n",
        "loaders = [pdf_loader, excel_loader, word_loader]\n",
        "documents = []\n",
        "for loader in loaders:\n",
        "    documents.extend(loader.load())\n",
        "\n",
        "# Chunk and Embeddings\n",
        "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
        "documents = text_splitter.split_documents(documents)\n",
        "\n",
        "embeddings = OpenAIEmbeddings()\n",
        "vectorstore = Chroma.from_documents(documents, embeddings)\n",
        "\n",
        "# Initialise Langchain - Conversation Retrieval Chain\n",
        "qa = ConversationalRetrievalChain.from_llm(ChatOpenAI(temperature=0), vectorstore.as_retriever())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6QYJxTi-wH5M",
        "outputId": "ce11f5af-023d-440e-e19b-fddb7ab4db22"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain.text_splitter:Created a chunk of size 1225, which is longer than the specified 1000\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 1042, which is longer than the specified 1000\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 1270, which is longer than the specified 1000\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 1266, which is longer than the specified 1000\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 1115, which is longer than the specified 1000\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 1112, which is longer than the specified 1000\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 1420, which is longer than the specified 1000\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 1504, which is longer than the specified 1000\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 1412, which is longer than the specified 1000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chat_history = []\n",
        "user_message = \"get agreement\"\n",
        "response = qa({\"question\": user_message, \"chat_history\": chat_history})\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8fEH_l1twZ_n",
        "outputId": "65ff7e5e-5a36-4ac4-cac9-433ce56a7e99"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'question': 'get agreement', 'chat_history': [], 'answer': \"I'm sorry, I need more information to answer your question. Which agreement are you referring to? Can you please provide me with an agreement ID or policy number?\"}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "x7T4PrqQwTE2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "metadata": {
        "id": "oVSUG882ZbG7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('Not using a high-RAM runtime')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ],
      "metadata": {
        "id": "X1UzcPv6H_qe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸŒ¿ exploring some idea with **Towhee**"
      ],
      "metadata": {
        "id": "yz20tRhe6E78"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install towhee"
      ],
      "metadata": {
        "id": "dQWnuluV6Kh0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from towhee import AutoPipes"
      ],
      "metadata": {
        "id": "83oZuDaj6KqV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "p = AutoPipes.pipeline('sentence_embedding')\n",
        "output = p('Hello World.').get()\n",
        "print(output)"
      ],
      "metadata": {
        "id": "nT7ejK2z6K-a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from towhee import pipe, ops\n",
        "\n",
        "img_embedding = (\n",
        "    pipe.input('url')\n",
        "        .map('url', 'img', ops.image_decode.cv2())\n",
        "        .map('img', 'embedding', ops.image_embedding.timm(model_name='resnet50'))\n",
        "        .output('embedding')\n",
        ")\n",
        "\n",
        "url = 'https://github.com/towhee-io/towhee/raw/main/towhee_logo.png'\n",
        "res = img_embedding(url).get()"
      ],
      "metadata": {
        "id": "fN6Z4eGV8i1y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(res)"
      ],
      "metadata": {
        "id": "eISbN1Jo8wNR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "quit()"
      ],
      "metadata": {
        "id": "YRf1aA7h6-jh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}