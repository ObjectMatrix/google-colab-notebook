{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ObjectMatrix/google-colab-notebook/blob/main/vicuna_13b_gpu.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KJwsonNSg0Ki"
      },
      "source": [
        "First, let's install the FastChat (Vicuna) repository."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "_LPmr_8e-zf7"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/thisserand/FastChat.git\n",
        "%cd FastChat\n",
        "!pip install -e ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6BAFoMDg9XR"
      },
      "source": [
        "Second, we will install the GPTQ-for-LLaMA repository which allows us to use GPTQ quantised models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "jyfx1xgw-6Dd"
      },
      "outputs": [],
      "source": [
        "!mkdir repositories\n",
        "%cd repositories\n",
        "!git clone https://github.com/oobabooga/GPTQ-for-LLaMa.git -b cuda\n",
        "%cd GPTQ-for-LLaMa\n",
        "!python setup_cuda.py install\n",
        "%cd ../.."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "awYGokkLhMRx"
      },
      "source": [
        "Okay, now we will download the quantised model weights."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "DaXDRfxW_AMt"
      },
      "outputs": [],
      "source": [
        "!python download-model.py anon8231489123/vicuna-13b-GPTQ-4bit-128g"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "26jSC2H2hTvE"
      },
      "source": [
        "To be able to run the web UI inside Colab we need virtual terminals. For this, we will use the screen library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "OtnynrQ7_Ex8"
      },
      "outputs": [],
      "source": [
        "!sudo apt-get install screen"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4DYLjOJlhdKB"
      },
      "source": [
        "Launch the controler service in the background."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "2733J4ri_FXw"
      },
      "outputs": [],
      "source": [
        "!screen -S controller -dm python -m fastchat.serve.controller --host \"127.0.0.1\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Th8dvPf6hhQL"
      },
      "source": [
        "Launch the model worker in the background."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ScpIxPq8AfRE"
      },
      "outputs": [],
      "source": [
        "!screen -S model_worker -dm python -m fastchat.serve.model_worker --model-path anon8231489123/vicuna-13b-GPTQ-4bit-128g --model-name vicuna-gptq --wbits 4 --groupsize 128 --host \"127.0.0.1\" --worker-address \"http://127.0.0.1:21002\" --controller-address \"http://127.0.0.1:21001\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TbKeBsebEBkj"
      },
      "source": [
        "Wait a little bit (~30 seconds) before you run this cell to make sure that the model worker is up and running. Using the public URL you can interact with the Vicuna model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p1G8Gh04AsRT"
      },
      "outputs": [],
      "source": [
        "!python -m fastchat.serve.gradio_web_server --controller-url \"http://127.0.0.1:21001\" --share"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}