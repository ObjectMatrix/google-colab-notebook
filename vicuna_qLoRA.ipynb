{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "authorship_tag": "ABX9TyNjvm2w4vSPyKLD79vXrCt9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ObjectMatrix/google-colab-notebook/blob/main/vicuna_qLoRA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mQvTXNuZ9ZUy"
      },
      "outputs": [],
      "source": [
        "!pip install -q -U bitsandbytes\n",
        "!pip install -q -U git+https://github.com/huggingface/transformers.git\n",
        "!pip install -q -U git+https://github.com/huggingface/peft.git\n",
        "#!pip install -q -U git+https://github.com/huggingface/accelerate.git\n",
        "#current version of Accelerate on GitHub breaks QLoRa\n",
        "#Using standard pip instead\n",
        "!pip install -q -U accelerate\n",
        "!pip install -q -U datasets!\n",
        "# !pip install llama_cpp\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "\n",
        "# from llama_cpp import Llama\n",
        "# import random"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig"
      ],
      "metadata": {
        "id": "OZN4e4et925G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "# model_name = \"/content/drive/MyDrive/models/vicuna-7b-1.1.ggmlv3.q4_0.bin\"\n",
        "model_name = \"EleutherAI/gpt-neox-20b\"\n",
        "\n",
        "#Tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n"
      ],
      "metadata": {
        "id": "Cq2nDnhN_F1W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- load_in_4bit: The model will be loaded in the memory with 4-bit precision.\n",
        "\n",
        "- bnb_4bit_use_double_quant: We will do the double quantization proposed by QLoRa.\n",
        "\n",
        "- bnb_4bit_quant_type: This is the type of quantization. “nf4” stands for 4-bit NormalFloat.\n",
        "\n",
        "- bnb_4bit_compute_dtype: While we load and store the model in 4-bit, we will partially dequantize it when needed and do all the computations with a 16-bit precision (bfloat16)."
      ],
      "metadata": {
        "id": "WQk41cPhYw1x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "quant_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")"
      ],
      "metadata": {
        "id": "fUMmjbK6YvrD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So now we can load the model in 4-bit:"
      ],
      "metadata": {
        "id": "8B2UdLP5ZIVa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=quant_config, device_map={\"\":0})"
      ],
      "metadata": {
        "id": "he4sTZhPZJMB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, we enable gradient checkpointing:"
      ],
      "metadata": {
        "id": "SXe6JumxZXDl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.gradient_checkpointing_enable()"
      ],
      "metadata": {
        "id": "CcEMSoovZX1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preprocessing the GPT model for LoRa\n",
        "This is where we use PEFT. We prepare the model for LoRa, adding trainable adapters for each layer."
      ],
      "metadata": {
        "id": "zpq7v83yZe_p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model\n",
        "\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\"query_key_value\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, config)"
      ],
      "metadata": {
        "id": "ydYkbATfZgXZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In LoraConfig, you can play with r, alpha, and dropout to obtain better results on your task. You can find more options and details in the PEFT repository.\n",
        "\n",
        "With LoRa, we add only 8 million parameters. We will only train these parameters and freeze everything else. Fine-tuning should be fast.\n",
        "\n",
        "Get your dataset ready\n",
        "For this demo, I use the “english_quotes” dataset. This is a dataset made of famous quotes distributed under a CC BY 4.0 license.\n",
        "\n",
        "https://towardsdatascience.com/qlora-fine-tune-a-large-language-model-on-your-gpu-27bed5a03e2b"
      ],
      "metadata": {
        "id": "4UkfLprjaWhR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "data = load_dataset(\"Abirate/english_quotes\")\n",
        "data = data.map(lambda samples: tokenizer(samples[\"quote\"]), batched=True)"
      ],
      "metadata": {
        "id": "lLxj-8t5aXXE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fine-tuning GPT-NeoX-20B with QLoRa\n",
        "Finally, the fine-tuning with Hugging Face Transformers is very standard.\n",
        "\n"
      ],
      "metadata": {
        "id": "lurqIGePdFuS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "trainer = transformers.Trainer(\n",
        "    model=model,\n",
        "    train_dataset=data[\"train\"],\n",
        "    args=transformers.TrainingArguments(\n",
        "        per_device_train_batch_size=1,\n",
        "        gradient_accumulation_steps=8,\n",
        "        warmup_steps=2,\n",
        "        max_steps=20,\n",
        "        learning_rate=2e-4,\n",
        "        fp16=True,\n",
        "        logging_steps=1,\n",
        "        output_dir=\"outputs\",\n",
        "        optim=\"paged_adamw_8bit\"\n",
        "    ),\n",
        "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
        ")\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "1ZC0afGAdGyr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "GPT Inference with QLoRa\n",
        "The QLoRa model we fine-tuned can be directly used with the standard Hugging Face Transformers’ inference, as follows:"
      ],
      "metadata": {
        "id": "1XPEiEaPdO1m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Ask not what your country\"\n",
        "device = \"cuda:0\"\n",
        "inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
        "\n",
        "outputs = model.generate(**inputs, max_new_tokens=20)\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "id": "LJ57Eps0dPca"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}